# Linear Regression from Scratch

This project implements **Linear Regression** completely from scratch using Python and NumPy â€” without relying on machine learning libraries like scikit-learn. It helps understand the underlying mathematics, gradient descent optimization, and how models actually learn.

---

## ğŸ“Œ Project Overview
Linear Regression is one of the simplest machine learning algorithms. It models the relationship between a **dependent variable (y)** and an **independent variable (x)** by fitting a straight line.

The equation is:

\[
y = w \cdot x + b
\]

- **w** â†’ weight (slope of the line)  
- **b** â†’ bias (intercept)  
- **x** â†’ input feature  
- **y** â†’ predicted output  

The model learns `w` and `b` using **gradient descent** to minimize the **Mean Squared Error (MSE)** between predicted and actual values.

---

## ğŸš€ Features
- Linear Regression implemented from scratch  
- Gradient Descent optimization  
- Loss function tracking (MSE)  
- Visualization of regression line and error reduction  

---

## ğŸ“Š Visualization
- Scatter plot of data points  
- Best-fit regression line  
- Loss vs Epochs curve to show model convergence  

---

## ğŸ› ï¸ How It Works
1. Initialize weights and bias  
2. Predict outputs using the linear equation  
3. Compute the loss (MSE)  
4. Update weights & bias using gradient descent  
5. Repeat until convergence  

---

## ğŸ”® Future Improvements
- Extend to **Multiple Linear Regression**  
- Add **Polynomial Regression**  
- Support **Stochastic Gradient Descent (SGD)**  
- Compare with **scikit-learnâ€™s LinearRegression**  
- Add interactive **Jupyter Notebook demo**  

---

## ğŸ“Œ Learning Outcomes
- Understand the math behind linear regression  
- Learn how gradient descent updates parameters  
- Gain intuition about optimization in ML models  
